This is my NLP project at ETH Zurich. It's still work in progress.
Under Transformers, You can find all code relating to the transformer based on the original paper 'Attention is all you need'

TODO:
- Train Models (Write Train, Eval Code, Run it on CTB7 first)
- If Good, Then do for all datasets, otherwise, Debug, Fix Model, Repeat

